{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderWithCLS(nn.Module):\n",
    "    def __init__(self, embedder, embedding_size, num_heads, num_layers, num_classes):\n",
    "        super(TransformerEncoderWithCLS, self).__init__()\n",
    "\n",
    "        self.embedder = embedder\n",
    "\n",
    "        self.positional_embedding = nn.Embedding(10 * 250, embedding_size) # Positional embedding\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_size, num_heads),\n",
    "            num_layers\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_size))  # Learnable <cls> token\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier = nn.Linear(embedding_size, num_classes)  # Classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)  # Embed the input sequence\n",
    "        batch_size, seq_len, embedding_size = x.size()\n",
    "\n",
    "        # Add positional embedding\n",
    "        positions = torch.arange(0, seq_len).expand(batch_size, seq_len).to(device)\n",
    "        x = x + self.positional_embedding(positions)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, 1, embedding_size)  # Shape: (batch_size, 1, embedding_size)\n",
    "        x_cls = torch.cat([cls_tokens, x], dim=1)  # Shape: (batch_size, seq_length + 1, embedding_size)\n",
    "\n",
    "        output = self.transformer_encoder(x_cls)  # Apply TransformerEncoder\n",
    "        \n",
    "        out_cls = output[:, 0]  # Return the representation of the <cls> token\n",
    "\n",
    "        out_cls = self.activation(out_cls)  # Apply activation function\n",
    "\n",
    "        return self.classifier(out_cls)  # Classify the <cls> token representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "\n",
    "        # 1x1 convolution branch\n",
    "        self.branch1x1 = nn.Conv1d(in_channels, out_channels[0], kernel_size=1)\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels[1], kernel_size=1),\n",
    "            nn.Conv1d(out_channels[1], out_channels[2], kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels[3], kernel_size=1),\n",
    "            nn.Conv1d(out_channels[3], out_channels[4], kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        # Max pooling branch\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv1d(in_channels, out_channels[5], kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        # Concatenate the branch outputs along the channel dimension\n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        outputs = rearrange(outputs, 'b c s -> b s c')  # Convert back to (batch_size, seq_len, channels)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "incept = InceptionBlock(1, [16, 8, 16, 8, 16, 16])\n",
    "batch_size = 64\n",
    "freq = 250\n",
    "seq_len = 10 * freq\n",
    "in_size = 1\n",
    "x = torch.randn(batch_size, in_size, seq_len) \n",
    "embedding_size = 64\n",
    "\n",
    "transformer = TransformerEncoderWithCLS(incept, embedding_size, 8, 2, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─InceptionBlock: 1-1                         [-1, 2500, 64]            --\n",
      "|    └─Conv1d: 2-1                            [-1, 16, 2500]            32\n",
      "|    └─Sequential: 2-2                        [-1, 16, 2500]            --\n",
      "|    |    └─Conv1d: 3-1                       [-1, 8, 2500]             16\n",
      "|    |    └─Conv1d: 3-2                       [-1, 16, 2500]            400\n",
      "|    └─Sequential: 2-3                        [-1, 16, 2500]            --\n",
      "|    |    └─Conv1d: 3-3                       [-1, 8, 2500]             16\n",
      "|    |    └─Conv1d: 3-4                       [-1, 16, 2500]            656\n",
      "|    └─Sequential: 2-4                        [-1, 16, 2500]            --\n",
      "|    |    └─MaxPool1d: 3-5                    [-1, 1, 2500]             --\n",
      "|    |    └─Conv1d: 3-6                       [-1, 16, 2500]            32\n",
      "├─Embedding: 1-2                              [-1, 2500, 64]            160,000\n",
      "├─TransformerEncoder: 1-3                     [-1, 2501, 64]            --\n",
      "|    └─ModuleList: 2                          []                        --\n",
      "|    |    └─TransformerEncoderLayer: 3-7      [-1, 2501, 64]            281,152\n",
      "|    |    └─TransformerEncoderLayer: 3-8      [-1, 2501, 64]            281,152\n",
      "├─ReLU: 1-4                                   [-1, 64]                  --\n",
      "├─Linear: 1-5                                 [-1, 5]                   325\n",
      "===============================================================================================\n",
      "Total params: 723,781\n",
      "Trainable params: 723,781\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 4.51\n",
      "===============================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 88.23\n",
      "Params size (MB): 2.76\n",
      "Estimated Total Size (MB): 91.00\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─InceptionBlock: 1-1                         [-1, 2500, 64]            --\n",
       "|    └─Conv1d: 2-1                            [-1, 16, 2500]            32\n",
       "|    └─Sequential: 2-2                        [-1, 16, 2500]            --\n",
       "|    |    └─Conv1d: 3-1                       [-1, 8, 2500]             16\n",
       "|    |    └─Conv1d: 3-2                       [-1, 16, 2500]            400\n",
       "|    └─Sequential: 2-3                        [-1, 16, 2500]            --\n",
       "|    |    └─Conv1d: 3-3                       [-1, 8, 2500]             16\n",
       "|    |    └─Conv1d: 3-4                       [-1, 16, 2500]            656\n",
       "|    └─Sequential: 2-4                        [-1, 16, 2500]            --\n",
       "|    |    └─MaxPool1d: 3-5                    [-1, 1, 2500]             --\n",
       "|    |    └─Conv1d: 3-6                       [-1, 16, 2500]            32\n",
       "├─Embedding: 1-2                              [-1, 2500, 64]            160,000\n",
       "├─TransformerEncoder: 1-3                     [-1, 2501, 64]            --\n",
       "|    └─ModuleList: 2                          []                        --\n",
       "|    |    └─TransformerEncoderLayer: 3-7      [-1, 2501, 64]            281,152\n",
       "|    |    └─TransformerEncoderLayer: 3-8      [-1, 2501, 64]            281,152\n",
       "├─ReLU: 1-4                                   [-1, 64]                  --\n",
       "├─Linear: 1-5                                 [-1, 5]                   325\n",
       "===============================================================================================\n",
       "Total params: 723,781\n",
       "Trainable params: 723,781\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.51\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 88.23\n",
       "Params size (MB): 2.76\n",
       "Estimated Total Size (MB): 91.00\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(transformer, (1, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = transformer.to(device)\n",
    "x = x.to(device)\n",
    "out = transformer(x)\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "class SleepStageDataset(Dataset):\n",
    "    def __init__(self, subjects, data, labels, seq_len, freq):\n",
    "        '''\n",
    "        This class takes in a list of subject, a path to the MASS directory \n",
    "        and reads the files associated with the given subjects as well as the sleep stage annotations\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Get the sleep stage labels\n",
    "        self.full_signal = []\n",
    "        self.full_labels = []\n",
    "\n",
    "        self.subject_list = []\n",
    "        for subject in subjects:\n",
    "            if subject not in data.keys():\n",
    "                print(f\"Subject {subject} not found in the pretraining dataset\")\n",
    "                continue\n",
    "\n",
    "            # Get the signal for the given subject\n",
    "            signal = torch.tensor(data[subject]['signal'], dtype=torch.float)\n",
    "\n",
    "            # Get all the labels for the given subject\n",
    "            label = torch.tensor([SleepStageDataset.get_labels().index(lab) for lab in labels[subject]]).type(torch.uint8)\n",
    "\n",
    "            # Repeat the labels freq times to match the signal using a pytorch function\n",
    "            label = torch.tensor(label).repeat_interleave(freq) \n",
    "\n",
    "            # Add some '?' padding at the end to make sure the length of signal and label match\n",
    "            missing = len(signal) - len(label)\n",
    "            label = torch.cat([label, torch.full((missing, ), SleepStageDataset.get_labels().index('?')).type(torch.uint8)])\n",
    "\n",
    "            # Make sure that the signal and the labels are the same length\n",
    "            assert len(signal) == len(label)\n",
    "\n",
    "            # Add to full signal and full label\n",
    "            self.full_labels.append(label)\n",
    "            self.full_signal.append(signal)\n",
    "            del data[subject], signal, label\n",
    "        \n",
    "        self.full_signal = torch.cat(self.full_signal)\n",
    "        self.full_labels = torch.cat(self.full_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels():\n",
    "        return ['1', '2', '3', 'R', 'W', '?']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get data and label at the given index\n",
    "        signal = self.full_signal[index - self.seq_len:index]\n",
    "        label = self.full_labels[index]\n",
    "        signal = signal.unsqueeze(0)\n",
    "\n",
    "        return signal, label.type(torch.LongTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.full_signal)\n",
    "    \n",
    "class SleepStageSampler(Sampler):\n",
    "    def __init__(self, dataset, seq_len, nb_batch_per_epoch, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.seq_len = seq_len\n",
    "        self.max_len = len(dataset)\n",
    "        self.limit = nb_batch_per_epoch * batch_size\n",
    "        self.nb_batch_per_epoch = nb_batch_per_epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.limit): \n",
    "            while True:\n",
    "                index = random.randint(self.seq_len, self.max_len - 1)\n",
    "                # Make sure that the label at the end of the window is not '?'\n",
    "                label = self.dataset.full_labels[index]\n",
    "                if label != SleepStageDataset.get_labels().index('?'):\n",
    "                    break\n",
    "            yield index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nb_batch_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from portiloop_software.portiloop_python.ANN.utils import get_configs\n",
    "\n",
    "from portiloop_software.portiloop_python.ANN.data.mass_data import read_pretraining_dataset, read_sleep_staging_labels, read_spindle_trains_labels\n",
    "import torch\n",
    "\n",
    "\n",
    "experiment_name = 'test_sleep_staging'\n",
    "seed = 42\n",
    "\n",
    "config = get_configs(experiment_name, False, seed)\n",
    "# config['nb_conv_layers'] = 4\n",
    "# config['hidden_size'] = 64\n",
    "# config['nb_rnn_layers'] = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Run some testing on subject 1\n",
    "# Load the data\n",
    "labels = read_spindle_trains_labels(config['old_dataset'])\n",
    "ss_labels = read_sleep_staging_labels(config['path_dataset'])\n",
    "# for index, patient_id in enumerate(ss_labels.keys()):\n",
    "\n",
    "\n",
    "data = read_pretraining_dataset(config['MASS_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_842516/290790759.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).repeat_interleave(freq)\n"
     ]
    }
   ],
   "source": [
    "dataset = SleepStageDataset(list(data.keys()), data, ss_labels, seq_len, freq)\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SleepStageSampler(dataset, seq_len, 1000, batch_size),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 2500]), torch.Size([64]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(torch.argmax(model(X), dim=1).cpu().numpy())\n",
    "            loss = criterion(model(X), y)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += torch.sum(torch.argmax(model(X), dim=1) == y)\n",
    "            total_samples += len(X)\n",
    "    loss  = total_loss / len(data_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    class_report = classification_report(y_true, y_pred, target_names=SleepStageDataset.get_labels()[:-1])\n",
    "    return loss, accuracy, conf_mat, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_epoch(model, data_loader, loss_function, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    epoch_loss /= len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/portiloop-training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/project/portiloop-training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/project/portiloop-training/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-5)\n",
    "\n",
    "max_epochs = 100\n",
    "for i in range(max_epochs):\n",
    "    train_loss, train_acc = train_epoch(transformer, loader, criterion, optimizer, device)\n",
    "    test_loss, test_accuracy, conf_mat, class_report = evaluate(transformer, loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.96601337591807"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44903125"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.30902067820232"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4695, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00      5842\n",
      "           2       0.47      1.00      0.64     30051\n",
      "           3       0.00      0.00      0.00      8026\n",
      "           R       0.00      0.00      0.00     10858\n",
      "           W       0.00      0.00      0.00      9223\n",
      "\n",
      "    accuracy                           0.47     64000\n",
      "   macro avg       0.09      0.20      0.13     64000\n",
      "weighted avg       0.22      0.47      0.30     64000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
