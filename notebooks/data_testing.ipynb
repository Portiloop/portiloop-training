{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from portiloop_software.portiloop_python.ANN.utils import get_configs\n",
    "\n",
    "from portiloop_software.portiloop_python.ANN.data.mass_data import read_pretraining_dataset, read_sleep_staging_labels, read_spindle_trains_labels\n",
    "from portiloop_software.portiloop_python.ANN.models.lstm import get_trained_model\n",
    "import torch\n",
    "\n",
    "\n",
    "experiment_name = 'test_adapation'\n",
    "seed = 42\n",
    "model_path = 'no_att_baseline'\n",
    "subject_id = '01-02-0019'\n",
    "\n",
    "config = get_configs(experiment_name, False, seed)\n",
    "# config['nb_conv_layers'] = 4\n",
    "# config['hidden_size'] = 64\n",
    "# config['nb_rnn_layers'] = 4\n",
    "\n",
    "# Load the model\n",
    "net = get_trained_model(config, config['path_models'] / model_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Run some testing on subject 1\n",
    "# Load the data\n",
    "labels = read_spindle_trains_labels(config['old_dataset'])\n",
    "ss_labels = read_sleep_staging_labels(config['path_dataset'])\n",
    "# for index, patient_id in enumerate(ss_labels.keys()):\n",
    "\n",
    "config['subject_id'] = subject_id\n",
    "\n",
    "data = read_pretraining_dataset(config['MASS_dir'], patients_to_keep=[subject_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DataBuffer:\n",
    "    \"\"\"\n",
    "    A class to get the data in the right format for the model from a stream of data\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, window_size, seq_stride):\n",
    "        self.seq_len = seq_len\n",
    "        self.window_size = window_size\n",
    "        self.seq_stride = seq_stride\n",
    "\n",
    "        # Compute the total number of points to keep in memory as the buffer\n",
    "        buffer_size = (seq_len - 1) * seq_stride + window_size\n",
    "        self.data = torch.zeros(buffer_size, dtype=torch.float32)\n",
    "\n",
    "    def step(self, point):\n",
    "        # Shift the data\n",
    "        self.data[:-1] = self.data.clone()[1:]\n",
    "        self.data[-1] = point\n",
    "        current_data = self.data.clone().unfold(0, self.window_size, self.seq_stride)\n",
    "        current_data = current_data.unsqueeze(0).unsqueeze(2)\n",
    "        return current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from portiloop_software.portiloop_python.ANN.utils import RMSScorer\n",
    "import random\n",
    "\n",
    "class AdaptationSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Sample random items from a dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over the dataset\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            toss = random.random()\n",
    "            if toss > 0.5:\n",
    "                # Get a random index from the spindle indexes\n",
    "                yield 1\n",
    "            else:\n",
    "                yield 0\n",
    "\n",
    "\n",
    "class AdapatationDatasetRMS(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for adaptation using the RMS score metric. \n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, window_size, replacement=False, candidate_threshold=0.5, rms_threshold=2.5, real_threshold=0.95, buffer_time=1250):\n",
    "        self.buffer_time = buffer_time # 5 seconds: time before and after each window for the RMS\n",
    "        self.buffer_size = max(seq_len * window_size, self.buffer_time) + self.buffer_time\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seq_len = seq_len\n",
    "        self.window_size = window_size\n",
    "        self.candidate_threshold = candidate_threshold\n",
    "        self.rms_threshold = rms_threshold\n",
    "        self.real_threshold = real_threshold\n",
    "        self.scorer = RMSScorer()\n",
    "\n",
    "        self.replacement = replacement\n",
    "\n",
    "        self.interval = 100 # 400 ms\n",
    "\n",
    "        # List of samples\n",
    "        self.positive_samples = []\n",
    "        self.negative_samples = []\n",
    "        # self.spindle_indexes = []\n",
    "        # self.non_spindle_indexes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns a sample from the dataset. If replacement is True, the sample is not removed from the dataset.\n",
    "        '''\n",
    "        # Choose a random index in one of the lists\n",
    "        index_in_list = random.randint(0, len(self.positive_samples)-1) if index == 1 else random.randint(0, len(self.negative_samples)-1)\n",
    "\n",
    "        list_to_sample = self.positive_samples if index == 1 else self.negative_samples\n",
    "\n",
    "        sample = list_to_sample[index_in_list] if self.replacement else list_to_sample.pop(index_in_list)\n",
    "\n",
    "        return sample, index\n",
    "    \n",
    "    def spindle_percentage(self):\n",
    "        sum_spindles = sum([i[1] for i in self.samples if i[1] == 1])\n",
    "        return sum_spindles / len(self)\n",
    "\n",
    "    def add_sample(self, sample, label):\n",
    "        \"\"\"\n",
    "        Takes a sample and adds it to the dataset.\n",
    "        \"\"\"\n",
    "        # self.samples.append((sample, label))\n",
    "        # if label == 1:\n",
    "        #     self.spindle_indexes.append(len(self.samples) - 1)\n",
    "        # else:\n",
    "        #     self.non_spindle_indexes.append(len(self.samples) - 1)\n",
    "        if label == 1:\n",
    "            self.positive_samples.append(sample)\n",
    "        else:\n",
    "            self.negative_samples.append(sample)\n",
    "\n",
    "    def step(self, point, label):\n",
    "        # Shift the data\n",
    "        self.data.append(point)\n",
    "        self.labels.append(label)\n",
    "    \n",
    "    def compute(self, max_time=-1):\n",
    "        \"\"\"\n",
    "        Compute the RMS score for the whole buffer and add to the dataset\n",
    "        \"\"\"\n",
    "        # start = time.time()\n",
    "        # Filter the whole buffer\n",
    "        self.filtered_data = self.scorer.filter(self.data)\n",
    "\n",
    "        labels = np.array(self.labels)\n",
    "    \n",
    "        # Get all indexes where the labels are above the candidate threshold\n",
    "        indexes = np.where(labels >= self.candidate_threshold)[0]\n",
    "\n",
    "        # # Remove the indexes where the previous is less than 400 ms before\n",
    "        # indexes = indexes[np.insert(np.diff(indexes) >= self.interval, 0, True)]\n",
    "\n",
    "        # Get the RMS score for each index\n",
    "        rms_scores = []\n",
    "        rms_scores_detect = []\n",
    "        for i, index in enumerate(indexes):\n",
    "            if index - self.buffer_time < 0 or index + self.buffer_time >= len(self.data):\n",
    "                continue\n",
    "            score = self.scorer.get_score(self.filtered_data[index - self.buffer_time:index + self.buffer_time], filter=False)\n",
    "            rms_scores.append(score)\n",
    "            if labels[index] >= self.real_threshold and i == 0:\n",
    "                rms_scores_detect.append(score)\n",
    "            elif labels[index] >= self.real_threshold and index - indexes[i-1] >= self.interval:\n",
    "                rms_scores_detect.append(score)\n",
    "        rms_scores = np.array(rms_scores)\n",
    "\n",
    "        # Get the indexes where the RMS score is above the threshold\n",
    "        indexes_positive = indexes[np.where(rms_scores >= self.rms_threshold)[0]]\n",
    "\n",
    "        # Get the indexes where the RMS score is below the threshold\n",
    "        indexes_negative = indexes[np.where(rms_scores < self.rms_threshold)[0]]\n",
    "\n",
    "        # Add the positive samples to the dataset\n",
    "        for index in indexes_positive:\n",
    "            if index - self.seq_len * self.window_size < 0:\n",
    "                continue\n",
    "            ordered_data = torch.tensor(self.data[index-(self.seq_len * self.window_size):index]).clone().reshape(self.seq_len, 1, self.window_size)\n",
    "            self.add_sample(ordered_data, 1)\n",
    "\n",
    "        # Add the negative samples to the dataset\n",
    "        for index in indexes_negative:\n",
    "            if index - self.seq_len * self.window_size < 0:\n",
    "                continue\n",
    "            ordered_data = torch.tensor(self.data[index-(self.seq_len * self.window_size):index]).clone().reshape(self.seq_len, 1, self.window_size)\n",
    "            self.add_sample(ordered_data, 0)\n",
    "\n",
    "        # end = time.time()\n",
    "        # print(f\"Time to compute: {end - start}\")\n",
    "\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        return rms_scores_detect\n",
    "        \n",
    "\n",
    "    def num_samples(self):\n",
    "        '''\n",
    "        Returns the minimum between the positive and the negative samples\n",
    "        '''\n",
    "        return min(len(self.positive_samples), len(self.negative_samples)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20000/6598000 [00:02<12:52, 8516.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, 77 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m# Get the output of the network\u001b[39;00m\n\u001b[1;32m     64\u001b[0m h_zero \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((config[\u001b[39m'\u001b[39m\u001b[39mnb_rnn_layers\u001b[39m\u001b[39m'\u001b[39m], train_sample\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), config[\u001b[39m'\u001b[39m\u001b[39mhidden_size\u001b[39m\u001b[39m'\u001b[39m]), device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 65\u001b[0m output, _, _ \u001b[39m=\u001b[39m net_copy(train_sample, h_zero)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     68\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/project/portiloop-training/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/project/portiloop-training/portiloop_software/portiloop_python/ANN/models/lstm.py:180\u001b[0m, in \u001b[0;36mPortiloopNetwork.forward\u001b[0;34m(self, x, h, past_x, max_value)\u001b[0m\n\u001b[1;32m    178\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, in_channels, features)\n\u001b[1;32m    179\u001b[0m \u001b[39m# x = self.wavelet_cnn(x)\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(x)\n\u001b[1;32m    181\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn2(x)\n\u001b[1;32m    183\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, end_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/project/portiloop-training/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/project/portiloop-training/portiloop_software/portiloop_python/ANN/models/lstm.py:47\u001b[0m, in \u001b[0;36mConvPoolModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n\u001b[1;32m     48\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m/project/portiloop-training/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/project/portiloop-training/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/project/portiloop-training/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "train = True\n",
    "\n",
    "buffer = DataBuffer(config['seq_len'], config['window_size'], config['seq_stride'])\n",
    "dataset = AdapatationDatasetRMS(config['seq_len'], config['window_size'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'], sampler=AdaptationSampler(dataset))\n",
    "\n",
    "net_copy = copy.deepcopy(net)\n",
    "net_copy = net_copy.to(device)\n",
    "net_copy = net_copy.train()\n",
    "\n",
    "h1 = torch.zeros((config['nb_rnn_layers'], 1, config['hidden_size']), device=device)\n",
    "\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.AdamW(net_copy.parameters(), lr=config['lr_adam'], weight_decay=config['adam_w'])\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "\n",
    "rms_scores = []\n",
    "real_indexes = []\n",
    "\n",
    "signal = data[subject_id]['signal']\n",
    "output = 0\n",
    "\n",
    "for index, point in enumerate(tqdm(signal)):\n",
    "\n",
    "    current_data = buffer.step(point)\n",
    "\n",
    "    if index < config['seq_len'] * config['window_size']:\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "                \n",
    "        if index % config['seq_stride'] == 0:\n",
    "            # Get the output of the network if we have waited the seq stride steps\n",
    "            output, h1, _ = net_copy(current_data.to(device), h1)\n",
    "            output = output.squeeze(-1)\n",
    "            output = output[-1].item()\n",
    "\n",
    "        # # Put the data into the dataset\n",
    "        dataset.step(point, output)\n",
    "\n",
    "        # real_indexes.append(output)\n",
    "\n",
    "        if index % 10000 == 0:\n",
    "            rms_scores += dataset.compute()\n",
    "\n",
    "    # If we have enough data, we train the network\n",
    "    if dataset.num_samples() > config['batch_size'] and train:\n",
    "\n",
    "        print(f\"Training, {dataset.num_samples()} samples\")\n",
    "\n",
    "        train_sample, train_label = next(iter(dataloader))\n",
    "        train_sample = train_sample.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get the output of the network\n",
    "        h_zero = torch.zeros((config['nb_rnn_layers'], train_sample.size(0), config['hidden_size']), device=device)\n",
    "        output, _, _ = net_copy(train_sample, h_zero)\n",
    "        \n",
    "        # Compute the loss\n",
    "        output = output.squeeze(-1)\n",
    "        train_label = train_label.squeeze(-1).float()\n",
    "        loss = criterion(output, train_label)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.459782155121597, 2.504105168078916)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_scores = np.array(rms_scores)\n",
    "rms_scores.mean(), rms_scores.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4883720930232558"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rms_scores >= 3) / len(rms_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_indexes = np.append(np.zeros(2700), real_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_real_model = real_indexes >= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([6407992,  190008]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels_model, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([6406270,  191730]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels_real_model, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spindles: 73\n",
      "Number of spindle labels: 12410\n",
      "len of full signal: 6598000\n",
      "Length of sampler: 157094\n",
      "Doing index: 0/157094\n",
      "Doing index: 10000/157094\n",
      "Doing index: 20000/157094\n",
      "Doing index: 30000/157094\n",
      "Doing index: 40000/157094\n",
      "Doing index: 50000/157094\n",
      "Doing index: 60000/157094\n",
      "Doing index: 70000/157094\n",
      "Doing index: 80000/157094\n",
      "Doing index: 90000/157094\n",
      "Doing index: 100000/157094\n",
      "Doing index: 110000/157094\n",
      "Doing index: 120000/157094\n",
      "Doing index: 130000/157094\n",
      "Doing index: 140000/157094\n",
      "Doing index: 150000/157094\n",
      "Time:  239.07770156860352\n",
      "Distribution of the predictions:\n",
      "(array([0., 1.], dtype=float32), array([152570,   4524]))\n",
      "Distribution of the labels:\n",
      "(array([0., 1.], dtype=float32), array([156799,    295]))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from portiloop_software.portiloop_python.ANN.adaptation_training import run_adaptation\n",
    "from portiloop_software.portiloop_python.ANN.data.mass_data import SingleSubjectDataset, SingleSubjectSampler, read_pretraining_dataset\n",
    "from portiloop_software.portiloop_python.ANN.utils import get_metrics\n",
    "\n",
    "\n",
    "config['subject_id'] = subject_id\n",
    "\n",
    "data = read_pretraining_dataset(config['MASS_dir'], patients_to_keep=[subject_id])\n",
    "\n",
    "assert subject_id in data.keys(), 'Subject not in the dataset'\n",
    "assert subject_id in labels.keys(), 'Subject not in the dataset'\n",
    "\n",
    "dataset = SingleSubjectDataset(config['subject_id'], data=data, labels=labels, config=config, ss_labels=ss_labels)  \n",
    "sampler = SingleSubjectSampler(len(dataset), config['seq_stride'])\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=1, \n",
    "    sampler=sampler, \n",
    "    num_workers=0)\n",
    "\n",
    "# Run the adaptation\n",
    "start = time.time()\n",
    "# run_adaptation(dataloader, net, device, config)\n",
    "output_total, window_labels_total, loss, net_copy = run_adaptation(dataloader, net, device, config, train)\n",
    "end = time.time()\n",
    "print('Time: ', end - start)\n",
    "\n",
    "print(\"Distribution of the predictions:\")\n",
    "print(np.unique(output_total.cpu().numpy(), return_counts=True))\n",
    "print(\"Distribution of the labels:\")\n",
    "print(np.unique(window_labels_total.cpu().numpy(), return_counts=True))\n",
    "\n",
    "# Get the metrics\n",
    "acc, f1, precision, recall = get_metrics(output_total, window_labels_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_model = np.repeat(output_total.cpu().numpy(), 42)\n",
    "labels_model = np.append(labels_model, np.zeros(52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_indexes = labels_model == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_indexes = np.where(old_indexes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_indexes = old_indexes[np.insert(np.diff(old_indexes) > 100, 0, True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1695,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_indexes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6598000,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_model = np.diff(labels_model)\n",
    "labels_model[labels_model == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_391917/4203616244.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rms_scores = torch.tensor(rms_scores)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(3.4736, dtype=torch.float64), tensor(2.4870, dtype=torch.float64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_scores = torch.tensor(rms_scores)\n",
    "rms_scores.mean(), rms_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6690)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rms_scores > 2) / len(rms_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
